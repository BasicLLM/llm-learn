{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 深度学习常用库介绍\n",
    "\n",
    "> 君子生非异也，善假于物也。 -- 荀子 《劝学》\n",
    "\n",
    "---"
   ],
   "id": "2c6e05d509837ab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pytorch\n",
    "\n",
    "---\n",
    "\n",
    "在之前的 Transformer 实现过程中，虽然我们运用了众多 PyTorch 的类与函数来构建网络架构，但并未深入挖掘和充分利用 PyTorch 的全部功能，仅满足于实现神经网络的各类结构。经过多年的发展与完善，PyTorch 内部已经成功集成了诸多优秀的框架和模块。接下来，我们将着重介绍 PyTorch 内部一系列出色的模块，以便更充分地发挥其强大功能，提升神经网络的开发效率和性能表现。\n",
    "\n",
    "开源地址：[Github-pytorch/pytorch](https://github.com/pytorch/pytorch) \\\n",
    "官方文档：[Pytroch 官方文档](https://pytorch.org/docs/stable/index.html)"
   ],
   "id": "512e920580df956e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Flash Attention\n",
    "\n",
    "Flash Attention 是一种优化的注意力机制算法，通过分块计算和重计算技术显著提升计算速度，减少内存占用，保持计算精确性，支持更长序列处理，并充分利用硬件资源，适用于大规模 Transformer 模型和长序列建模场景。\n",
    "\n",
    "pytoch 自 2.0 版本以后就内部集成了 Flash Attention 相关的模块，你只需要使用 `torch.nn.functional.scaled_dot_product_attention()` 函数即可，下面只提供基础的演示。\n",
    "\n",
    "官方文档：[Pytorch 官方文档-torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)"
   ],
   "id": "d9bb880628c1a1fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =====================================\n",
    "batch = 3   # 数据批数\n",
    "seq_len = 6 # 序列长度\n",
    "dim_emb = 8 # 嵌入维度\n",
    "# =====================================\n",
    "\n",
    "q = torch.randn([batch, seq_len, dim_emb])\n",
    "k = torch.randn([batch, seq_len, dim_emb])\n",
    "v = torch.randn([batch, seq_len, dim_emb])\n",
    "\n",
    "output = F.scaled_dot_product_attention(q, k, v)\n",
    "print('output size: ', output.size())\n",
    "print('output: ', output)"
   ],
   "id": "64f868c399afeaad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Pytorch DDP\n",
    "\n",
    "PyTorch 的 **DistributedDataParallel（DDP）** 是一种高效的分布式数据并行训练框架，专为多 GPU 或多节点环境设计。其核心原理是将模型复制到每个计算设备（如 GPU），并在各设备间并行处理不同的数据分片：前向传播时，每个设备独立计算本地模型的输出和损失；反向传播后，DDP 通过高效的通信后端（如 NCCL）自动同步所有设备的梯度，确保模型参数全局一致。相较于传统的 DataParallel，DDP 采用多进程模式（每个 GPU 对应一个进程），避免了 Python 全局解释器锁（GIL）的限制，同时利用 Ring-AllReduce 等算法优化通信效率，显著提升了训练速度和扩展性，尤其适用于大规模数据集和深层模型。此外，DDP 支持动态图机制，与 PyTorch 生态无缝集成，并通过简明的 API 简化了分布式训练的复杂度，成为工业级深度学习训练的首选方案。\n",
    "\n",
    "官方文档：[Pytorch 官方文档-DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.)\n"
   ],
   "id": "b2e468a788d2d176"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Huggingface\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Transformers\n",
    "\n",
    "Hugging Face 的 Transformers 库是一个开源的 Python 工具库，专注于自然语言处理（NLP）任务，提供了大量基于 Transformer 架构的预训练模型（如 BERT、GPT、T5 等）以及简单易用的 API。该库通过模块化设计和统一接口（如 `AutoModel` 和 `AutoTokenizer`），简化了模型的加载、微调和推理流程，支持文本分类、问答、机器翻译、文本生成等多种任务。此外，它还与分布式训练框架（如 DeepSpeed 和 Accelerate）深度兼容，适合大规模模型的训练与部署。凭借其丰富的预训练模型库和活跃的开源社区支持，Transformers 库已成为大语言模型（LLM）开发的重要工具，降低了开发门槛并推动了 NLP 技术的普及。\n",
    "\n",
    "开源地址： [Github-huggingface/transformers](https://github.com/huggingface/transformers) \\\n",
    "官方文档： [huggingface 官方文档-transformers](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "### 2. TRL\n",
    "\n",
    "Hugging Face 的 **TRL（Transformer Reinforcement Learning）** 库是一个用于通过强化学习训练 Transformer 语言模型的全栈工具库。它支持多种强化学习方法，包括监督微调（SFT）、奖励建模（RM）、近端策略优化（PPO）和直接偏好优化（DPO），并提供了丰富的训练器类（如 `SFTTrainer`、`PPOTrainer` 和 `DPOTrainer`）来简化模型的微调和优化过程。\n",
    "\n",
    "TRL 库基于 Hugging Face 的 Transformers 库构建，兼容所有预训练模型架构，并利用加速器（如 accelerate）实现从单 GPU 到大规模多节点集群的高效扩展。它还集成了 PEFT 和 unsloth 等技术，以优化硬件资源利用，支持在资源受限的环境中训练大型模型。\n",
    "\n",
    "此外，TRL 提供了命令行界面（CLI）和自动模型类（如 AutoModelForCausalLMWithValueHead），使得模型训练和部署更加便捷。\n",
    "\n",
    "开源地址： [Github-huggingface/trl](https://github.com/huggingface/trl) \\\n",
    "官方文档： [huggingface 官方文档-trl](https://huggingface.co/docs/trl/index)"
   ],
   "id": "2f197601c6ae638"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
