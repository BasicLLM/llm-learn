{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GQA 分组查询注意力\n",
    "\n",
    "**GQA（Grouped-Query Attention）** 是Transformer中多头注意力机制的优化变体，它通过将多个查询头进行分组，并在每组内共享同一组键和值，从而在降低计算成本和内存占用的同时，较好地平衡了模型的性能和效率，尤其适用于长文本处理和大规模推理任务。\n",
    "\n",
    "## 详细原理图\n",
    "\n",
    "![GQA Struct](img/GQA-Struct.svg)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 代码实现\n",
    "\n",
    "---\n",
    "\n",
    "#### 引入必要的库"
   ],
   "id": "445eaa7eb6c897df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math"
   ],
   "id": "73a863f0ba26b22c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### KV 复制函数\n",
    "\n",
    "由于我们对注意力计算中的 Q、K、V 进行了分组，而每个分组中是由多个 Q 对应一个 KV，那么在做矩阵运算时，就会出现维度不一致的问题，这时我们就需要对 KV 进行复制，使其在维度上保持一致。如下图所示：\n",
    "\n",
    "![KV Repeat](img/KV-Repeat.svg)\n",
    "\n",
    "下面是 KV 复制函数的实现代码，其中 pytorch 也提供了 `torch.repeat_interleave(input,repeats,dim,output_size)` 函数用于张量的复制和扩充。"
   ],
   "id": "94fa13c433885704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def repeat_kv(x, n_rep):\n",
    "    \"\"\"\n",
    "    复制并扩充 KV 矩阵，等价于 `torch.repeat_interleave(x, dim=2, repeats=n_rep)`\n",
    "\n",
    "    :param x: 传入的数据\n",
    "    :param n_rep:  重复的数量\n",
    "    :return: 复制完成的矩阵\n",
    "    \"\"\"\n",
    "    batch, seq_len, kv_head_num, dim_head = x.size()\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(batch, seq_len, kv_head_num, n_rep, dim_head)\n",
    "        .reshape(batch, seq_len, kv_head_num * n_rep, dim_head)\n",
    "    )"
   ],
   "id": "8abec28d44a76868",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 分组注意力代码实现\n",
    "\n",
    "这里的实现将不再关注 Decoder 的传入数据，即实现的是自注意力机制。"
   ],
   "id": "bbe901c7a38f097a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, dim_emb:int, dim_head:int, dim_out:int, q_head_num:int, kv_head_num:int):\n",
    "        \"\"\"\n",
    "        Grouped Query Attention\n",
    "\n",
    "        :param dim_emb: 嵌入维度\n",
    "        :param dim_head: 头维度\n",
    "        :param dim_out: 输出维度\n",
    "        :param q_head_num: Q 头数\n",
    "        :param kv_head_num: KV 头数\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.dim_out = dim_out\n",
    "        self.q_head_num = q_head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "\n",
    "        # 分组的数量\n",
    "        self.group_num = q_head_num // kv_head_num\n",
    "\n",
    "        # 创建 Attention 权重\n",
    "        self.Q_net = nn.Linear(dim_emb, q_head_num * dim_head)\n",
    "        self.K_net = nn.Linear(dim_emb, kv_head_num * dim_head)\n",
    "        self.V_net = nn.Linear(dim_emb, kv_head_num * dim_head)\n",
    "\n",
    "        # 创建权重\n",
    "        self.W_net = nn.Linear(q_head_num * dim_head, dim_out)\n",
    "\n",
    "    def forward(self, x, mask_mat = None, use_cache = False, kv_cache = None):\n",
    "\n",
    "        batch,seq_len,_ = x.size()\n",
    "\n",
    "        # 计算 qkv 的值\n",
    "        # q   的维度为 (batch, seq_len, q_head_num  , dim_head)\n",
    "        # k/v 的维度为 (batch, seq_len, kv_head_num , dim_head)\n",
    "        q = self.Q_net(x).view(batch, seq_len, self.q_head_num , self.dim_head)\n",
    "        k = self.K_net(x).view(batch, seq_len, self.kv_head_num, self.dim_head)\n",
    "        v = self.V_net(x).view(batch, seq_len, self.kv_head_num, self.dim_head)\n",
    "\n",
    "        # q_head_num 与 kv_head_num 是不一致的，将其复制后使其维度保持一致\n",
    "        # qkv 的维度为 (batch, q_head_num, seq_len, dim_head)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = repeat_kv(k, self.group_num).transpose(1, 2)\n",
    "        v = repeat_kv(v, self.group_num).transpose(1, 2)\n",
    "\n",
    "        # 如果使用 KV Cache，拼接历史 K 和 V\n",
    "        if use_cache and kv_cache is not None:\n",
    "            k_prev, v_prev = kv_cache\n",
    "            # 在序列长度维度拼接\n",
    "            k = torch.cat([k_prev, k], dim=2)\n",
    "            v = torch.cat([v_prev, v], dim=2)\n",
    "\n",
    "        # 记录新的 KV Cache（供下一步使用）\n",
    "        new_kv_cache = (k, v) if use_cache else None\n",
    "\n",
    "        # 将 K 的最后两个维度进行转置，转置后的维度为 (batch, q_head_num, dim_head, seq_len)\n",
    "        k_t = k.transpose(-1, -2)\n",
    "\n",
    "        # 计算 qk^T / sqrt(d_k)，此时 s0 的维度为 (batch, q_head_num, seq_len, seq_len)\n",
    "        s0 = torch.matmul(q, k_t) / math.sqrt(self.q_head_num)\n",
    "\n",
    "        # 进行掩码遮掩操作\n",
    "        if mask_mat is not None:\n",
    "            # 进行遮掩\n",
    "            s0 = torch.masked_fill(\n",
    "                s0,\n",
    "                mask_mat,\n",
    "                float('-inf')\n",
    "            )\n",
    "\n",
    "        # 计算 softmax(s)*v ，此时 s1 的维度为 (batch, q_head_num, seq_len, dim_head)\n",
    "        s1 = torch.matmul(F.softmax(s0, dim=-1), v)\n",
    "\n",
    "        # 我们需要使用 s1*W ，这里就要将矩阵变换为可以跟 W 矩阵进行矩阵乘法的维度，即：\n",
    "        # s1 变换维度为：(batch, seq_len, dim_head * q_head_num)\n",
    "        s1 = s1.transpose(1, 2).contiguous() # 这里需要让内存连续（ 使用 reshape 则不用）\n",
    "        s1 = s1.view(batch, seq_len, self.q_head_num * self.dim_head)\n",
    "\n",
    "        # 输出的最终维度为：(batch, seq_len, dim_out)\n",
    "        output = self.W_net(s1)\n",
    "\n",
    "        return output, new_kv_cache"
   ],
   "id": "50cc49c3842f0924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型测试\n",
    "\n",
    "---"
   ],
   "id": "19b6ac84c9cfecdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 配置\n",
    "# =====================================\n",
    "batch = 3\n",
    "seq_len = 8\n",
    "dim_emb = 72\n",
    "q_head_num = 24\n",
    "kv_head_num = 6\n",
    "dim_head = dim_emb // q_head_num\n",
    "dim_out = dim_emb\n",
    "# =====================================\n",
    "\n",
    "attention = GroupedQueryAttention(dim_emb, dim_head, dim_emb, q_head_num, kv_head_num)\n",
    "\n",
    "# ===============\n",
    "#  模拟第一次调用\n",
    "# ===============\n",
    "\n",
    "# 模拟第一次传入的 X，这个 seq_len 可以不用限制\n",
    "X = torch.randn((batch, seq_len, dim_emb))\n",
    "\n",
    "# 生成不包含对角线的上三角矩阵（设置 diagonal=1）\n",
    "mask_mat = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# 第一次输出\n",
    "output, kv_cache = attention(X, mask_mat, use_cache = True)\n",
    "k_cache, v_cache = kv_cache\n",
    "\n",
    "# 打印输出及 KV Cache 维度信息\n",
    "print(f'output size : {output.size()}')\n",
    "print(f'k cache size : {k_cache.size()}')\n",
    "print(f'v cache size : {v_cache.size()}')\n",
    "\n",
    "# ===============\n",
    "#  模拟第二次调用\n",
    "# ===============\n",
    "\n",
    "# 模拟生成下一个 token\n",
    "# 注意此时需要 seq = 1，因为使用 KV Cache 只需传入下一个生成的 token 即可\n",
    "X_next = torch.randn(batch, 1, dim_emb)\n",
    "\n",
    "# 第二次调用（携带缓存），此时不用传入掩码矩阵\n",
    "output_next, kv_cache = attention(X_next, use_cache=True, kv_cache=kv_cache)\n",
    "k_cache_next, v_cache_next = kv_cache\n",
    "\n",
    "# 打印第二次调用的输出及 KV Cache 维度信息\n",
    "print(f'output size : {output_next.size()}')\n",
    "print(f'next k cache size : {k_cache_next.size()}')\n",
    "print(f'next v cache size : {v_cache_next.size()}')"
   ],
   "id": "f516d726b075701c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
