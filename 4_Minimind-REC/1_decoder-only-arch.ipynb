{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 实现 Decoder Only 架构的 Transformer 模型\n",
    "\n",
    "Decoder Only 架构是 Transformer 模型的一种变体，**仅保留 Transformer 中的解码器（Decoder）部分**，并对其进行调整和优化。这种架构在自然语言处理（NLP）领域被广泛用于生成式任务（如文本生成、对话系统等），典型代表是 GPT 系列模型（如 GPT-3、ChatGPT）。以下是基于 Transformer 变体架构的对比表：\n",
    "\n",
    "| 架构类型            |    典型模型    | 适配任务 | 与大模型的关系             |\n",
    "|:----------------|:----------:|:-------:|:--------------------|\n",
    "| Decoder Only    | GPT、LLaMA  | 生成任务（文本生成、对话） | 主流大模型的首选架构          |\n",
    "| Encoder Only    |    BERT    | 理解任务（分类、NER） | 参数量通常较小，不适合生成任务     |\n",
    "| Encoder-Decoder |  T5、BART   | 序列到序列任务（翻译、摘要）| 参数量较大但复杂度高，大模型中较少采用 |\n",
    "\n",
    "## 详细结构图\n",
    "\n",
    "![Decoder Only Arch](img/Decoder-Only-Arch.svg)\n",
    "\n",
    "---\n",
    "\n",
    "## 代码实现\n",
    "\n",
    "---"
   ],
   "id": "df3bad8f54aee6b4"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder import *\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer_num,\n",
    "            vocab_size,\n",
    "            dim_emb,\n",
    "            dim_head,\n",
    "            head_num,\n",
    "            max_seq_len:int = 50000,\n",
    "            training:bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Decoder Only Transformer\n",
    "\n",
    "        :param layer_num: 层数\n",
    "        :param vocab_size: 词库大小\n",
    "        :param dim_emb: 嵌入维度大小\n",
    "        :param dim_head: 单个注意力模块的头维度\n",
    "        :param head_num: 注意力头数\n",
    "        :param max_seq_len: 最大序列长度\n",
    "        :param training: 是否开启训练模式\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.head_num = head_num\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.training = training\n",
    "\n",
    "        # 位置编码模块\n",
    "        self.position_embedding = PositionalEmbedding(dim_emb, max_len = max_seq_len)\n",
    "\n",
    "        # 创建词嵌入层\n",
    "        self.word_embedding = nn.Embedding(vocab_size, dim_emb)\n",
    "\n",
    "        # 创建解码器\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(self.layer_num):\n",
    "            self.decoders.append(Decoder(dim_emb, dim_head, head_num))\n",
    "\n",
    "        # 创建线性层，线性层的作用是将嵌入维度转换为词表维度\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask = None, dec_kv_caches = None):\n",
    "\n",
    "        batch, seq_len = x.size()\n",
    "        training = self.training\n",
    "\n",
    "        # 输出词向量嵌入\n",
    "        embedded_x = self.word_embedding(x)\n",
    "\n",
    "        # 计算解码器掩码矩阵\n",
    "        if seq_len > 1:\n",
    "            triu_mask_mat = torch.triu(\n",
    "                torch.ones(seq_len, seq_len, dtype=torch.bool, device=x.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "            if mask is not None:\n",
    "                dec_mask_mat = torch.stack([triu_mask_mat | (~ mask[i].bool()) for i in range (batch)]).type(dtype=torch.bool)\n",
    "                dec_mask_mat = dec_mask_mat.view(batch, 1, seq_len, seq_len).requires_grad_(False)\n",
    "            else:\n",
    "                dec_mask_mat = triu_mask_mat.requires_grad_(False)\n",
    "        else:\n",
    "            dec_mask_mat = None\n",
    "\n",
    "        # 检查是否需要缓存\n",
    "        new_dec_kv_caches = [] if not training else None\n",
    "        if training:\n",
    "            # 位置编码\n",
    "            pe_x = self.position_embedding(embedded_x, 0, seq_len)\n",
    "\n",
    "            # 计算解码器的计算结果，其最终输出维度为 (batch, seq_len, dim_emb)\n",
    "            for decoder in self.decoder_blocks:\n",
    "                pe_x, _ = decoder(\n",
    "                    pe_x, pe_x, pe_x,\n",
    "                    dec_mask_mat = dec_mask_mat\n",
    "                )\n",
    "        else:\n",
    "            # 位置编码\n",
    "            if dec_kv_caches is not None:\n",
    "                # dec_kv_caches 对应的索引： [编码器层数][编码器中的多头注意力的层数][K Cache/V Cache]\n",
    "                # K Cache / V Cache 对应的张量维度： (批数, 头数, 缓存的 K / V 序列长度, 头维度)\n",
    "                cached_seq_len = dec_kv_caches[0][0][0].size(2)\n",
    "                pe_x = self.position_embedding(embedded_x, cached_seq_len, cached_seq_len + 1)\n",
    "            else:\n",
    "                pe_x = self.position_embedding(embedded_x, 0, seq_len)\n",
    "\n",
    "            # 计算解码器的计算结果，并缓存 KV 的值\n",
    "            for dec_idx, decoder in enumerate(self.decoders):\n",
    "                if dec_kv_caches is not None:\n",
    "                    kv_caches = dec_kv_caches[dec_idx]\n",
    "                else:\n",
    "                    kv_caches = None\n",
    "\n",
    "                pe_x, new_kv_caches = decoder(\n",
    "                    pe_x, pe_x, pe_x,\n",
    "                    dec_mask_mat = dec_mask_mat,\n",
    "                    use_cache = True,\n",
    "                    kv_caches = kv_caches\n",
    "                )\n",
    "                new_dec_kv_caches.append(new_kv_caches)\n",
    "\n",
    "        # 最终输出维度为 (batch, output_seq_len, vocab_size)\n",
    "        logits = self.linear(pe_x)\n",
    "        if not training:\n",
    "            output = F.softmax(logits, dim=-1)\n",
    "            return output, new_dec_kv_caches\n",
    "        else:\n",
    "            # 需要使用交叉熵计算损失，而 pytorch 提供的交叉熵算法内部已经包含了 softmax\n",
    "            # 所以这里结果不要进行 softmax ，否则会导致损失计算不稳定\n",
    "            return logits"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型测试\n",
    "\n",
    "---"
   ],
   "id": "d5feb8dd9fa5ecf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# 配置\n",
    "# =====================================\n",
    "layer_num = 6    # 编码器数量\n",
    "vocab_size = 256 # 词表大小\n",
    "dim_emb = 128    # 词向量维度\n",
    "dim_head = 32    # 注意力头维度\n",
    "head_num = 4     # 注意力头数\n",
    "\n",
    "batch = 3        # 测试数据批数\n",
    "seq_len = 12     # 测试输入数据长度\n",
    "# =====================================\n",
    "\n",
    "# 创建 transformer 网络\n",
    "transformer = DecoderOnlyTransformer(layer_num, vocab_size, dim_emb, dim_head, head_num)\n",
    "\n",
    "def rand_seq_code(batch, seq_len):\n",
    "    \"\"\"\n",
    "    随机序列编码\n",
    "\n",
    "    :param batch: 批数\n",
    "    :param seq_len: 序列长度\n",
    "    :return: 随机序列编码\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for i in range(batch):\n",
    "        bat_data = [random.randint(0, vocab_size) for _ in range(seq_len)]\n",
    "        data.append(bat_data)\n",
    "    return torch.tensor(data)\n",
    "\n",
    "# 构建随机输入输出序列\n",
    "x = rand_seq_code(batch, seq_len)\n",
    "mask = torch.stack([torch.arange(0,seq_len) for _ in range(batch)]) < 8\n",
    "\n",
    "# ===============\n",
    "#  模拟第一次调用\n",
    "# ===============\n",
    "\n",
    "# transformer 输出\n",
    "output, kv_caches = transformer(\n",
    "    x, mask\n",
    ")\n",
    "\n",
    "# 打印输出结果\n",
    "print(f'output size  : {output.size()}')\n",
    "print(f'kv cache len : {len(kv_caches)}')\n",
    "\n",
    "# 预测下一个值\n",
    "token = torch.argmax(output[:, -1, :], dim=-1)\n",
    "print(f'prediction token : {token}')\n",
    "\n",
    "# ===============\n",
    "#  模拟第二次调用\n",
    "# ===============\n",
    "\n",
    "# transformer 输出\n",
    "output_next, kv_caches_next = transformer(\n",
    "    x = token.view(batch, 1),\n",
    "    dec_kv_caches = kv_caches\n",
    ")\n",
    "token_next = torch.argmax(output_next[:, -1, :], dim=-1)\n",
    "print(f'next prediction token : {token_next}')"
   ],
   "id": "778ba85c4b9af19d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
