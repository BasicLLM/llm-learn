{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 基于 Transformer 的英译中翻译模型训练实践\n",
    "\n",
    "在前文中，我们已经实现了一个优化版本的 Transformer，这为我们接下来的训练实践奠定了坚实的基础。现在，我们将把目光聚焦于 Transformer 的训练过程，以典型的英译中翻译任务为案例，深入实践训练 Transformer 模型的整个流程。\n",
    "\n",
    "推荐观看： [CSDN-理解Transformer](https://blog.csdn.net/weixin_44878336/article/details/142485944)\n",
    "\n",
    "---"
   ],
   "id": "1ee404716c6ab95b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 基础知识\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 什么是分词器？\n",
    "\n",
    "分词器（tokenizer）的任务是把自然语言文本分解成一个个离散的单元，比如单词、子词，甚至是字符。它的核心目标是让模型能“看懂”文本的基本结构。比如，面对“Hi, 世界你好”这样的句子，tokenizer 会把它拆成`Hi`、`,`、`世界`、`你好` 这样的 token，并为每个 token 分配一个唯一的 ID。这些 ID 是模型能处理的数字形式，但它们本身并没有语义信息，只是符号。如：\n",
    "\n",
    "**原文：** `Hi, 世界你好` \\\n",
    "**分词：** `Hi = 12`、`, = 9`、`世界 = 63`、`你好 = 28` \\\n",
    "**序列：** `[12,9,63,28]`\n",
    "\n",
    "### 2. 什么是词嵌入？\n",
    "\n",
    "词嵌入（Word Embedding）的任务是把这些符号转化为语义化的向量。它通过嵌入矩阵，把每个 token 的 ID 映射到一个固定维度的向量空间中。比如，在一个 4 维的嵌入矩阵中（实际上使用的维度要大得多），“good”会被映射成一个 4 维的向量，这个向量不仅包含了“good”这个单词的语义信息，还能反映它与其他单词的关系，比如“good”和“nice”可能会在向量空间中更接近。如：\n",
    "\n",
    "$$\n",
    "E_{good} = \\begin{bmatrix}\n",
    " 0.23 \\\\\n",
    " 0.26 \\\\\n",
    " 0.66 \\\\\n",
    " 0.43 \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "E_{nice} = \\begin{bmatrix}\n",
    " 0.19 \\\\\n",
    " 0.25 \\\\\n",
    " 0.60 \\\\\n",
    " 0.46 \\\\\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "||E_{good} - E_{nice} || = 0.0787\n",
    "$$\n",
    "\n",
    "分词器和词嵌入的关系，就像是一场接力赛的两棒：一棒负责把文本拆解成有意义的“块”，另一棒则把这些“块”转化为能让模型理解的“语言”。它们的分工明确，但又紧密相连，共同决定了模型对文本的理解能力。"
   ],
   "id": "206a5b996ca4f67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 相关库引入\n",
    "\n",
    "---\n",
    "\n",
    "在本章我们不考虑如何训练一个 Tokenizer 而是直接引入 huggingface 的 `AutoTokenizer` 库。\n",
    "\n",
    "**文档：** [huggingface-AutoTokenizer](https://huggingface.co/docs/transformers/v4.49.0/en/model_doc/auto#transformers.AutoTokenizer)"
   ],
   "id": "e13ccaaa59917214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from safetensors.torch import load_file\n",
    "from pandarallel import pandarallel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 数据路径\n",
    "data_path = './dataset/wmt_zh_en_training_corpus.csv'\n",
    "\n",
    "# 测试标志\n",
    "run_test_cell = False"
   ],
   "id": "d20c3e9de56f7857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. 使用 Qwen-tokenizer\n",
    "\n",
    "Qwen-tokenizer是Qwen模型的分词器，其采用字节级字节对编码（BBPE，Byte-Level Byte-Pair Encoding）。BBPE从UTF-8字节开始构建词汇表，初始词汇表包含所有256个可能的字节。然后，通过迭代合并训练数据中最频繁出现的字节对，形成更长的子词，直到词汇表达到预设的大小。例如，一个常见的中文字符可能被合并为一个token，而不是拆分为多个字节。\n",
    "\n",
    "本地 `./tokenizer` 目录下存放了已经下载好的 Qwen-tokenizer ，可以直接使用。资源下载地址：[huggingface-Qwen-tokenizer](https://huggingface.co/Qwen/Qwen-tokenizer/tree/main)"
   ],
   "id": "efcc3d05af79db0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_test_cell:\n",
    "    # 加载预训练模型的分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./tokenizer/Qwen-tokenizer')\n",
    "\n",
    "    # vocab 词表长度\n",
    "    print(f'tokenizer vocab size: {tokenizer.vocab_size}')\n",
    "\n",
    "    # 测试分词器\n",
    "    test_text = \"你好，世界！\"\n",
    "    print(f'\\\"{test_text}\\\" 的分词结果为：',tokenizer(test_text)['input_ids'])"
   ],
   "id": "4845721bfe3ee518",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Qwen-tokenizer 特殊 token\n",
    "\n",
    "|                      token                      |  token ID   | 描述                                                 |\n",
    "|:-----------------------------------------------:|:-----------:|:---------------------------------------------------|\n",
    "|             <&#124;endoftext&#124;>             |   151643    | 表示文本结束的特殊字符，在 Qwen-7B 和 Qwen-7B-Chat 中用于标记文本的结束位置。 |\n",
    "|             <&#124;im_start&#124;>              |   151644    | 在 Qwen-7B-Chat 中用于标记交互的开始。                         |\n",
    "|              <&#124;im_end&#124;>               |   151645    | 在 Qwen-7B-Chat 中用于标记交互的结束。                         |\n",
    "| <&#124;extra_0&#124;> ~ <&#124;extra_204&#124;> |      -      | 这些是备用的特殊字符，开发者可以根据需要使用它们来实现特定的功能或扩展模型的能力。          |\n",
    "\n",
    "事实上上述特殊 token 都是给 Qwen-7B 及其相关系列模型使用的，我们不需要使用这些特殊 token ，对于接下来的训练我们需要自定义三个特殊的 token ：\n",
    "- `<bos>` : 用来标记序列开始\n",
    "- `<eos>` : 用来标记序列结束\n",
    "- `<pad>` : 补全标记，将序列处理成特定长度"
   ],
   "id": "5e10e76b50e81887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    './tokenizer/Qwen-tokenizer',\n",
    "    trust_remote_code=True,\n",
    "    bos_token='<bos>',  # 151646\n",
    "    eos_token='<eos>',  # 151647\n",
    "    pad_token='<pad>',  # 151648\n",
    ")"
   ],
   "id": "4380ae8087649bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**测试特殊 token**",
   "id": "77611914525f19b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_test_cell:\n",
    "\n",
    "    # 增加其他特殊字符\n",
    "    custom_tokenizer.add_tokens([\"<think>\"], special_tokens=True)\n",
    "\n",
    "    # 未定义特殊字符的情况\n",
    "    print(tokenizer(\"<bos>你好，世界<eos><pad><pad><pad><pad>\")['input_ids'])\n",
    "\n",
    "    # 定义特殊字符的情况\n",
    "    print(custom_tokenizer(\"<bos>你好，世界<eos><pad><pad><pad><pad>\")['input_ids'])\n",
    "\n",
    "    sentences = [\n",
    "        '<bos>你好，世界<eos>' ,\n",
    "        '<bos>什么是变形金刚<eos>' ,\n",
    "    ]\n",
    "\n",
    "    print(custom_tokenizer.batch_encode_plus(sentences))\n",
    "    print(custom_tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        max_length=30,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )['input_ids'])"
   ],
   "id": "ee5bc79056954f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 构建数据集\n",
    "\n",
    "---"
   ],
   "id": "573955ff64255407"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. 下载中英翻译语料数据集\n",
    "\n",
    "请将下载好的相关语料数据存放在本地 `./dataset` 目录下，资源下载地址：[魔塔-WMT中英机器翻译训练集](https://modelscope.cn/datasets/iic/WMT-Chinese-to-English-Machine-Translation-Training-Corpus/files)\n"
   ],
   "id": "c074778a9dd64dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 文件读取测试\n",
    "if run_test_cell:\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    print(df.head())  # 查看前几行数据\n",
    "    print(df.describe())  # 数据统计描述"
   ],
   "id": "366f422123ca81e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 处理语料数据集\n",
    "\n",
    "处理数据语料需要按照 huggingface 的 `Dataset` 标准来处理，具体来说，就是一列的处理结果都要用字典的形式返回，如下面的 `data_preprocess(cols, tokenizer, dec_seq_len)` 函数。"
   ],
   "id": "bddc72a16327581c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def data_preprocess(cols, tokenizer, dec_seq_len):\n",
    "    \"\"\"\n",
    "    数据预处理\n",
    "\n",
    "    :param cols: 传入的列\n",
    "    :param tokenizer: 分词器\n",
    "    :param dec_seq_len: 解码器序列长度\n",
    "    :return: (英文序列，中文序列，预测输出序列)\n",
    "    \"\"\"\n",
    "\n",
    "    cn_seqs = tokenizer.batch_encode_plus(\n",
    "        cols['0'],\n",
    "        max_length=dec_seq_len + 1,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    cn_seqs_ids = cn_seqs['input_ids']\n",
    "    cn_seqs_mask = cn_seqs['attention_mask']\n",
    "\n",
    "    en_seqs = tokenizer.batch_encode_plus(\n",
    "        cols['1'],\n",
    "        max_length=dec_seq_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_seq\": en_seqs['input_ids'],\n",
    "        \"input_mask\": en_seqs['attention_mask'],\n",
    "        \"output_seq\": [row[:dec_seq_len] for row in cn_seqs_ids],\n",
    "        \"output_mask\": [row[:dec_seq_len] for row in cn_seqs_mask],\n",
    "        \"labels\": [row[1:] for row in cn_seqs_ids],\n",
    "    }\n",
    "\n",
    "def load_dataset_from_csv(data_path, tokenizer, dec_seq_len, sample = None):\n",
    "    \"\"\"\n",
    "    数据预处理\n",
    "\n",
    "    :param data_path: 数据路径\n",
    "    :param tokenizer: 分词器\n",
    "    :param dec_seq_len: 解码器序列长度\n",
    "    :param sample: 采样数量\n",
    "    :return: (英文序列，中文序列，预测输出序列)\n",
    "    \"\"\"\n",
    "\n",
    "    # 启用 pandarallel 并行\n",
    "    pandarallel.initialize()\n",
    "\n",
    "    # 读取并加载数据\n",
    "    df = pd.read_csv(data_path)\n",
    "    if sample is not None:\n",
    "        df = df.sample(n=sample)\n",
    "\n",
    "    # 对中文列去除字符串中的空格，并添加开始和结束的 token\n",
    "    df['0'] = df['0'].parallel_apply(lambda x: f'{tokenizer.bos_token}{str(x).replace(\" \", \"\")}{tokenizer.eos_token}')\n",
    "\n",
    "    # 转换为 Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # 预处理数据\n",
    "    hf_dataset = hf_dataset.map(\n",
    "        lambda x : data_preprocess(x, tokenizer, dec_seq_len),\n",
    "        batched=True,\n",
    "        remove_columns=[\"0\", \"1\"]\n",
    "    )\n",
    "\n",
    "    # 设置数据集格式和所需要的列\n",
    "    hf_dataset.set_format(type='torch', columns=[\"input_seq\", \"output_seq\", \"input_mask\", \"output_mask\", 'labels'])\n",
    "\n",
    "    return hf_dataset"
   ],
   "id": "6e7a4454d646135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**测试数据集读取**",
   "id": "ea2c9ed7b6cf03bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_test_cell:\n",
    "\n",
    "    dec_seq_len = 30\n",
    "\n",
    "    # 通过 csv 加载\n",
    "    dataset = load_dataset_from_csv(data_path, custom_tokenizer, dec_seq_len, sample=320_000)\n",
    "\n",
    "    # 存储处理好的数据\n",
    "    # dataset.save_to_disk('./dataset/wmt_320000')\n",
    "\n",
    "    # 直接加载已经处理好的数据\n",
    "    # dataset = Dataset.load_from_disk('./dataset/wmt_10000')\n",
    "\n",
    "    # 测试\n",
    "    data = dataset.__getitem__(10)\n",
    "    en_seq = data['input_seq']\n",
    "    en_seq_mask = data['input_mask']\n",
    "    cn_seq = data['output_seq']\n",
    "    cn_seq_mask = data['output_mask']\n",
    "    labels = data['labels']\n",
    "\n",
    "    print(f'en_seq size: {en_seq.size()}')\n",
    "    print(f'en_seq_mask size: {en_seq_mask.size()}')\n",
    "    print(f'cn_seq size: {cn_seq.size()}')\n",
    "    print(f'cn_seq_mask size: {cn_seq_mask.size()}')\n",
    "    print(f'labels size: {labels.size()}')\n",
    "\n",
    "    print('en : ', custom_tokenizer.decode(en_seq.type(torch.int)))\n",
    "    print('en mask', en_seq_mask)\n",
    "    print('cn : ', custom_tokenizer.decode(cn_seq.type(torch.int)))\n",
    "    print('cn mask', cn_seq_mask)\n",
    "    print('pr : ', custom_tokenizer.decode(labels.type(torch.int)))"
   ],
   "id": "f5369457f242383a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 训练 Transformer\n",
    "\n",
    "---\n",
    "\n",
    "由于训练神经网络的具体实现方法都是大致相同的，区别点就是损失函数的定义，我们可以直接使用 huggingface 的 `transformers` 库提供的 `Trainer` 类来训练模型，该类可以通过设置参数或继承的方式来实现自定义损失函数，本次我们采用了定义了损失函数的方法： `compute_loss_func(outputs, labels, num_items_in_batch)` 。\n",
    "\n",
    "参考文档：[huggingface-Trainer.compute_loss_func](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/trainer#transformers.Trainer.compute_loss_func)\n",
    "\n",
    "### 1. 训练模型"
   ],
   "id": "313b6898e3a2364e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformer import Transformer\n",
    "\n",
    "one_hot_len = custom_tokenizer.vocab_size + len(custom_tokenizer.all_special_tokens) + 3\n",
    "pad_token_id = custom_tokenizer.encode(custom_tokenizer.pad_token)[0]\n",
    "\n",
    "# 配置\n",
    "# =====================================\n",
    "## 模型参数\n",
    "encoder_num = 6                             # 编码器数量\n",
    "decoder_num = 6                             # 解码器数量\n",
    "vocab_size = one_hot_len                    # 词表大小\n",
    "dim_emb = 512                               # 词向量维度\n",
    "dim_head = 64                               # 注意力头维度\n",
    "head_num = 8                                # 注意力头数\n",
    "continue_train = False                      # 是否继续之前的训练\n",
    "weights_path = \"./model/model.safetensors\"  # 如果继续之前的训练，需要指定权重的位置\n",
    "\n",
    "## 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # 模型保存路径\n",
    "    eval_strategy=\"epoch\",             # 每个 epoch 进行评估\n",
    "    save_strategy=\"best\",              # 保存最优结果\n",
    "    learning_rate=1e-4,                # 学习率\n",
    "    per_device_train_batch_size=32,    # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=64,     # 每个设备上的评估批次大小\n",
    "    num_train_epochs=20,               # 训练轮数\n",
    "    weight_decay=0.01,                 # 权重衰减\n",
    "    logging_dir=\"./logs\",              # 日志路径\n",
    "    logging_steps=10,                  # 日志记录步数\n",
    "    load_best_model_at_end=True,       # 训练结束时加载最优模型\n",
    "    label_names=['labels'],            # 设置标签名\n",
    ")\n",
    "# =====================================\n",
    "\n",
    "# 加载数据集\n",
    "# dataset = load_dataset_from_csv(data_path, custom_tokenizer, dec_seq_len)\n",
    "dataset = Dataset.load_from_disk(\"./dataset/wmt_10000\")\n",
    "\n",
    "# 按 80% 训练集和 20% 测试集的比例划分\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# 模型定义\n",
    "en_cn_translate_model = Transformer(encoder_num, decoder_num, vocab_size, dim_emb, dim_head, head_num)\n",
    "en_cn_translate_model.training = True\n",
    "\n",
    "# 是否继续之前的训练\n",
    "if continue_train:\n",
    "    # 直接加载为状态字典\n",
    "    state_dict = load_file(weights_path)\n",
    "    en_cn_translate_model.load_state_dict(state_dict)\n",
    "\n",
    "# 定义损失函数\n",
    "def compute_loss_func(outputs, labels, num_items_in_batch):\n",
    "\n",
    "    # 测试损失\n",
    "    if num_items_in_batch is None:\n",
    "        output,_,_ = outputs\n",
    "\n",
    "        # 对 softmax 输出取对数，得到 log-probs\n",
    "        log_probs = torch.log(output)\n",
    "\n",
    "        # 使用 NLLLoss 计算损失\n",
    "        loss = F.nll_loss(\n",
    "            log_probs.view(-1, log_probs.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=pad_token_id\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    # 训练损失\n",
    "    loss = F.cross_entropy(\n",
    "        outputs.view(-1, outputs.size(-1)),\n",
    "        labels.view(-1),\n",
    "        ignore_index=pad_token_id\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "#设置训练器\n",
    "trainer = Trainer(\n",
    "    model=en_cn_translate_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_loss_func=compute_loss_func\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./model\")"
   ],
   "id": "ebd936c265752c41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. 测试模型",
   "id": "17247d77f87e0b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformer import Transformer\n",
    "\n",
    "if run_test_cell:\n",
    "\n",
    "    custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        './tokenizer/Qwen-tokenizer',\n",
    "        trust_remote_code=True,\n",
    "        bos_token='<bos>',  # 151646\n",
    "        eos_token='<eos>',  # 151647\n",
    "        pad_token='<pad>',  # 151648\n",
    "    )\n",
    "    one_hot_len = custom_tokenizer.vocab_size + len(custom_tokenizer.all_special_tokens) + 3\n",
    "\n",
    "    # 设置配置\n",
    "    encoder_num = 6           # 编码器数量\n",
    "    decoder_num = 6           # 解码器数量\n",
    "    vocab_size = one_hot_len  # 词表大小\n",
    "    dim_emb = 512             # 词向量维度\n",
    "    dim_head = 32             # 注意力头维度\n",
    "    head_num = 16             # 注意力头数\n",
    "\n",
    "    # 初始化空模型\n",
    "    model = Transformer(encoder_num, decoder_num, vocab_size, dim_emb, dim_head, head_num)\n",
    "\n",
    "    # 加载 .safetensors 权重文件\n",
    "    weights_path = \"./model/model.safetensors\"\n",
    "\n",
    "    # 直接加载为状态字典\n",
    "    state_dict = load_file(weights_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    #####\n",
    "\n",
    "    en_text = 'How are you'\n",
    "    token = '<bos>'\n",
    "\n",
    "    # 进行编码\n",
    "    input_seq = custom_tokenizer.batch_encode_plus(\n",
    "        [en_text], return_attention_mask = False\n",
    "    )['input_ids']\n",
    "    print(input_seq)\n",
    "    input_seq = torch.tensor(input_seq)\n",
    "    output_seq = torch.tensor(custom_tokenizer.encode(token)).view(1, -1)\n",
    "\n",
    "    kv_caches = None\n",
    "    enc_output_cache = None\n",
    "\n",
    "    line = ''\n",
    "    for _ in range(30):\n",
    "        output, enc_output_cache ,kv_caches = model(\n",
    "            input_seq = input_seq,\n",
    "            output_seq = output_seq,\n",
    "            enc_output_cache = enc_output_cache,\n",
    "            dec_kv_caches = kv_caches\n",
    "        )\n",
    "        token_id = torch.argmax(output, dim = -1)\n",
    "        output_seq = token_id.view(1, 1)\n",
    "        token = custom_tokenizer.decode(token_id.view(-1))\n",
    "        line += token\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "\n",
    "    print(line)"
   ],
   "id": "99cd8c3207c4f8a5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
