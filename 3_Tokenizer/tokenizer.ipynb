{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 实现一个简单的 Tokenizer\n",
    "\n",
    "在第二部分我们简单的介绍了一下分词器的作用，但当时我们使用的是 Qwen 已经训练好的分词器，本章将介绍如何训练一个简单的分词器。\n",
    "\n",
    "---"
   ],
   "id": "d8a1b227f1c19089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 准备工作\n",
    "\n",
    "---\n",
    "\n",
    "引入必要的包与定义必要的常量"
   ],
   "id": "7192b0caabcb1cd0"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "import os\n",
    "import json"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**加载数据集**\n",
    "\n",
    "本地 `.\\dataset\\wikitext-2-raw-v1` 目录下已经下载了训练所需的数据集，可以直接使用。资源下载地址：[huggingface-wikitext/wikitext-2-raw-v1](https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-2-raw-v1)"
   ],
   "id": "da079b533044066e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载训练集\n",
    "dataset = load_dataset(\"parquet\", data_files={\n",
    "    'train': './dataset/wikitext-2-raw-v1/train-00000-of-00001.parquet',\n",
    "    'validation': './dataset/wikitext-2-raw-v1/validation-00000-of-00001.parquet',\n",
    "    'test': './dataset/wikitext-2-raw-v1/test-00000-of-00001.parquet'\n",
    "})\n",
    "\n",
    "# 打印数据\n",
    "# print(dataset['train'][:10])\n",
    "\n",
    "# 构建数据迭代器\n",
    "train_dataset = dataset['train']\n",
    "batch_size = 1000\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        yield train_dataset[i : i + batch_size][\"text\"]"
   ],
   "id": "79b84b4c150a4bc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**保存 Tokenizer**\n",
    "\n",
    "此代码保存为兼容 `AutoTokenizer` 的分词器格式。无需关注其实现。"
   ],
   "id": "feaf2165f257df6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def token_decoder_dict(token_decoder, export_special = True):\n",
    "\n",
    "    decoder_dict =  {\n",
    "        \"content\" : token_decoder.content,\n",
    "        \"lstrip\" : token_decoder.lstrip,\n",
    "        \"rstrip\" : token_decoder.rstrip,\n",
    "        \"normalized\" : token_decoder.normalized,\n",
    "        \"single_word\" : token_decoder.single_word,\n",
    "    }\n",
    "\n",
    "    if export_special:\n",
    "        decoder_dict[\"special\"] = token_decoder.special\n",
    "\n",
    "    return decoder_dict\n",
    "\n",
    "def save_tokenizer(\n",
    "        tokenizer,\n",
    "        save_path,\n",
    "        special_tokens,\n",
    "        tokenizer_class = None,\n",
    "        bos_token = None,\n",
    "        eos_token = None,\n",
    "        pad_token = None,\n",
    "        unk_token = None\n",
    "):\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # 保存分词器\n",
    "    tokenizer.save(f\"{save_path}/tokenizer.json\")\n",
    "\n",
    "    # 保存词汇表\n",
    "    with open(f\"{save_path}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tokenizer.get_vocab(), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 读取 JSON 文件\n",
    "    with open(f\"{save_path}/tokenizer.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        tokenizer_data = json.load(file)\n",
    "\n",
    "    # 保存 merges\n",
    "    model_data = tokenizer_data['model']\n",
    "    if 'merges' in model_data:\n",
    "        with open(f\"{save_path}/merges.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for merge in model_data['merges']:\n",
    "                f.write(f'{merge[0]} {merge[1]}\\n')\n",
    "\n",
    "    # 保存附加 token 表\n",
    "    added_tokens = {}\n",
    "    for added_token in special_tokens:\n",
    "        added_tokens[added_token] = tokenizer.get_vocab()[added_token]\n",
    "    with open(f\"{save_path}/added_tokens.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(added_tokens, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 获取分词器的 Token 解码器\n",
    "    tokenizer_added_tokens_decoder = tokenizer.get_added_tokens_decoder()\n",
    "\n",
    "    # 设置\n",
    "    config = {}\n",
    "    special_tokens_map = {}\n",
    "    def add_special_token(name ,special_token):\n",
    "        if special_token is not None:\n",
    "            config[name] = special_token\n",
    "            idx = tokenizer.get_vocab()[special_token]\n",
    "            special_tokens_map[name] = token_decoder_dict(tokenizer_added_tokens_decoder[idx], False)\n",
    "\n",
    "    add_special_token('bos_token', bos_token)\n",
    "    add_special_token('eos_token', eos_token)\n",
    "    add_special_token('unk_token', unk_token)\n",
    "    add_special_token('pad_token', pad_token)\n",
    "\n",
    "    if tokenizer_class is None:\n",
    "        config[\"tokenizer_class\"] = \"PreTrainedTokenizerFast\"\n",
    "    else:\n",
    "        config[\"tokenizer_class\"] = tokenizer_class\n",
    "\n",
    "    if special_tokens is not None:\n",
    "        added_tokens_decoder = {}\n",
    "        for key in tokenizer_added_tokens_decoder:\n",
    "            token_decoder = tokenizer_added_tokens_decoder[key]\n",
    "            added_tokens_decoder[key] = token_decoder_dict(token_decoder)\n",
    "        config['added_tokens_decoder'] = added_tokens_decoder\n",
    "\n",
    "    with open(f\"{save_path}/special_tokens_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open(f\"{save_path}/tokenizer_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=4)\n"
   ],
   "id": "f39db97f913dd0c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenizer 算法\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Byte-Pair Encoding (BPE)\n",
    "\n",
    "**原理：** 通过迭代合并高频字符对生成子词词汇表，例如将 `\"l o w\"` 和 `\"l o w e r\"` 合并为 `low` 和 `lower`。\n",
    "\n",
    "**特点：**\n",
    "- 贪心合并，适合高频词和常见子词。\n",
    "- 词汇表大小可自定义（例如 10k-100k）。\n",
    "\n",
    "**应用模型：** GPT 系列（如 GPT-2、GPT-3）、RoBERTa。\n",
    "\n",
    "**优势：**\n",
    "- 高效处理未知词（如 `\"chatting\"` → `\"chat\" + \"ting\"`）。\n",
    "- 简单且计算速度快。"
   ],
   "id": "37d3bb680175d9f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**使用 BPE 算法训练 Tokenizer**",
   "id": "b75f41104ad8c5da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "bos_token = '<bos>'\n",
    "eos_token = '<eos>'\n",
    "vocab_size = 20000\n",
    "\n",
    "# 特殊 Token\n",
    "special_tokens = [unk_token, pad_token, bos_token, eos_token]\n",
    "\n",
    "# 实例化一个 BPE 分词器\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n",
    "\n",
    "# 定义 BpeTrainer\n",
    "trainer = trainers.BpeTrainer()\n",
    "\n",
    "# 设置预分词器，这里使用简单的空格分词\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 添加特殊 token\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# 训练分词器\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "\n",
    "# 保存训练好的分词器\n",
    "save_tokenizer(\n",
    "    tokenizer,\n",
    "    save_path =\"tokenizer/bpe-tokenizer\",\n",
    "    special_tokens = special_tokens,\n",
    "    bos_token = bos_token,\n",
    "    eos_token = eos_token,\n",
    "    pad_token = pad_token,\n",
    "    unk_token = unk_token,\n",
    ")\n",
    "\n",
    "# 加载tokenizer\n",
    "# bpe_tokenizer = Tokenizer.from_file(\"./tokenizer/bpe-tokenizer/tokenizer.json\")"
   ],
   "id": "7b8376768774c45d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**测试 BEP Tokenizer**",
   "id": "8b86d3d7a66c15a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer/bpe-tokenizer\", trust_remote_code=True)\n",
    "\n",
    "# 编解码测试\n",
    "token_encode = bpe_tokenizer(\"<bos>How are you ?<eos>\", padding=\"max_length\", max_length=10)\n",
    "print('token_encode: ', token_encode)\n",
    "\n",
    "token_seq = bpe_tokenizer.decode(token_encode['input_ids'])\n",
    "print('token_seq: ', token_seq)"
   ],
   "id": "2d6d8f8536fa19af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. WordPiece\n",
    "\n",
    "WordPiece 是一种基于子词（subword）的分词算法，由谷歌在2016年提出，是BERT等主流预训练模型的分词核心。其基本思想是将单词拆分成更小的子词单元，以减少词表的大小，同时解决未登录词（OOV）的问题。以下是WordPiece分词器算法的简要介绍：\n",
    "\n",
    "**原理**\n",
    "- **核心思想：** WordPiece算法通过不断合并相邻的子词对来构建词表，每次选择合并的子词对是基于概率计算的，即选择合并后能最大化语言模型概率的子词对。具体来说，计算两个相邻子词的互信息值（PMI），选择具有最大PMI值的子词对进行合并。\n",
    "- **与BPE算法的区别：** BPE算法是基于子词对的频率来选择合并的子词对，而WordPiece算法则更进一步，考虑了子词对在语言模型中的关联性，使得合并后的子词对在语义上更连贯。\n",
    "\n",
    "**分词过程**\n",
    "\n",
    "- **训练阶段：**\n",
    "    1. **初始化：** 将语料库中的所有单词以字符为单位进行拆分，每个字符作为一个子词，初始化词表为所有可能的字符集合。\n",
    "    2. **统计与合并：** 遍历每个单词中所有可能的子词对，统计它们的出现频数，计算每个子词对的得分（如PMI值），选择得分最高的子词对进行合并，将合并后的子词加入词表，并记录合并规则。\n",
    "    3. **迭代：** 重复上述统计与合并步骤，直到词表达到预设的大小。\n",
    "- **分词阶段：** 利用训练好的词表，对输入的单词进行正向最长匹配分词，即不断寻找单词中的最长前缀子词。\n",
    "\n",
    "**优势**\n",
    "- **减少词表大小：** 通过将单词拆分成子词，可以有效减少词表的大小，同时保留常见词的完整性。\n",
    "- **解决未登录词问题：** 对于未在训练数据中出现的单词，可以将其拆分成已存在的子词组合，从而避免了未登录词的问题。\n",
    "- **提高模型泛化能力：** WordPiece算法能够更好地捕捉子词之间的语义关联，有助于提高模型在不同语料上的泛化能力。\n",
    "\n",
    "WordPiece分词器算法在自然语言处理领域得到了广泛应用，特别是在基于Transformer的预训练模型中，为模型的训练和应用提供了有效的文本预处理手段。"
   ],
   "id": "e2923200f994bfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "bos_token = '<bos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "# 特殊 Token\n",
    "special_tokens = [unk_token, pad_token, bos_token, eos_token]\n",
    "\n",
    "# 实例化一个 BPE 分词器\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=unk_token))\n",
    "\n",
    "# 定义 BpeTrainer 并设置参数\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    special_tokens = special_tokens,\n",
    "    vocab_size = vocab_size,\n",
    "    min_frequency = 2,\n",
    "    show_progress = True\n",
    ")\n",
    "\n",
    "# 设置预分词器，这里使用简单的空格分词\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 添加特殊 token\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# 训练分词器\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "\n",
    "# 保存训练好的分词器\n",
    "save_tokenizer(\n",
    "    tokenizer,\n",
    "    save_path =\"tokenizer/wordpiece-tokenizer\",\n",
    "    special_tokens = special_tokens,\n",
    "    bos_token = bos_token,\n",
    "    eos_token = eos_token,\n",
    "    pad_token = pad_token,\n",
    "    unk_token = unk_token,\n",
    ")"
   ],
   "id": "79e77180f8211726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**测试 WordPiece Tokenizer**",
   "id": "5aac0e580a43ba87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer/wordpiece-tokenizer\", trust_remote_code=True)\n",
    "\n",
    "# 编解码测试\n",
    "token_encode = bpe_tokenizer(\"<bos>How are you ?<eos>\", padding=\"max_length\", max_length=10)\n",
    "print('token_encode: ', token_encode)\n",
    "\n",
    "token_seq = bpe_tokenizer.decode(token_encode['input_ids'])\n",
    "print('token_seq: ', token_seq)"
   ],
   "id": "51c2b1dc721d9488",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
