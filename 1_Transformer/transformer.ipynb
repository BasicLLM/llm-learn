{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 实现一个典型的 Transformer\n",
    "\n",
    "在实现 Transformer 之前，需要简单的 pytorch 和深度学习相关的知识，推荐观看：\n",
    "\n",
    "(1) [动手学深度学习（第二版）](https://zh.d2l.ai/) \\\n",
    "(2) [GPT是什么？直观解释Transformer](https://www.bilibili.com/video/BV13z421U7cs) \\\n",
    "(3) [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "\n",
    "本文的核心讨论聚焦于算法代码实现层面，针对 Transformer 各部分模块的原理将不展开系统性论述，但会在相应章节为深度技术解析提供参考资料。\n",
    "\n",
    "## 详细原理图\n",
    "\n",
    "![Transformer Struct](img/Transformer-Struct.svg)\n",
    "\n",
    "如图所示，实现一个典型的 Transformer 模型通常需要实现下面三个神经网络模块：\n",
    "- **RMSNorm :** 方根误差归一化\n",
    "- **Feed Forward :** 前馈神经网络\n",
    "- **Muti-Head Attention :** 多头注意力\n",
    "\n",
    "其中多头注意力机制依赖 **Scaled Dot-product Attention** (缩放点积注意力) 的实现。\n",
    "\n",
    "此外，我们还要实现下面的函数：\n",
    "- **Potitional Encoding Function :** 位置编码函数，用来标识输入数据的顺序。\n",
    "- **Mask Matrix Generate Function :** 掩码矩阵生成函数，用来生成掩码矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "## 准备工作\n",
    "\n",
    "---\n",
    "\n",
    "引入必要的库与定义必要的常量"
   ],
   "id": "5e978bce3c3e3f63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math"
   ],
   "id": "26edca423a0e639b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 三个神经网络模块的实现\n",
    "\n",
    "---\n",
    "\n",
    "### 1. RMSNorm\n",
    "\n",
    "Root Mean Squared Error Normalized, 即均方根误差归一化。\n",
    "\n",
    "#### (1) 计算均方差根（RMS）\n",
    "$$\n",
    "RMS(x) = \\sqrt{\\frac{1}{d}\\sum^{d}_{i = 0}{x_i^{2}}+\\epsilon}\n",
    "$$\n",
    "\n",
    "#### (2) 归一化输出向量\n",
    "$$\n",
    "\\hat{x} = \\frac{x}{RMS(x)}\n",
    "$$\n",
    "\n",
    "#### (3) 应用缩放参数\n",
    "$$\n",
    "RMSNorm(x) = \\gamma \\odot \\hat{x}\n",
    "$$\n",
    "\n",
    "### 解释\n",
    "- $x$ 是输入向量，维度为 $d$。\n",
    "- $\\epsilon$ 是一个很小的数，用于防止除零。\n",
    "- $\\gamma$ 是可学习的缩放参数，维度为 $d$。\n",
    "- $\\odot$ 表示元素级的乘法操作。\n"
   ],
   "id": "7c312dcf467290b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim:int, eps:float=1e-5):\n",
    "        \"\"\"\n",
    "        Root Mean Squared Error Normalized\n",
    "\n",
    "        :param dim: 维度\n",
    "        :param eps: 公式中的 ε，很小的数，防止除零\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # weight 即缩放矩阵的权重\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _rms(self, x):\n",
    "        # rsqrt 是 sqrt 的倒数，即 rsqrt(x) = 1/sqrt(x)\n",
    "        # 这里 mean() 中,dim = -1 ，指对最后一维求平均数，而 keepdim 指不改变该张量的维度\n",
    "        return torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * self._rms(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ],
   "id": "4e5e50f6b83420d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Feed Forward\n",
    "\n",
    "前馈神经网络，就是一个两层的全连接神经网络。"
   ],
   "id": "73986ddbfacc46f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim:int, dim_hidden:int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim_hidden, bias=False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_hidden, dim, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return self.net(x)"
   ],
   "id": "d919052d37f253ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Attention\n",
    "\n",
    "#### (1) 实现缩放点积注意力 (Scaled Dot-product Attention) 神经网络模块\n",
    "\n",
    "一个经典的注意力机制的网络结构如下所示：\n",
    "\n",
    "![Classic Attention Struct](img/Classic-Attention-Struct.svg)\n",
    "\n",
    "图中的 Q 表示 **查询（Query）** ，K 表示 **键（Key）** ，V 表示 **值（Value）** 。 注意力输出公式如下：\n",
    "\n",
    "$$\n",
    "Attention(x) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "Q = XW_q\n",
    "$$\n",
    "$$\n",
    "K = XW_k\n",
    "$$\n",
    "$$\n",
    "V = XW_v\n",
    "$$\n",
    "\n",
    "在这里的 $d_{k}$ 我们取输出层的维度（图中的 $D_{out}$），如下图所示：\n",
    "\n",
    "![QKV Compute](./img/QKV-Compute.png)\n",
    "\n",
    "有关 $d_{k}$ 取值的相关解释，推荐参考：[知乎-transformer中的attention为什么scaled?](https://www.zhihu.com/question/339723385/answer/3513306407)"
   ],
   "id": "1c23969d34c960af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_emb:int, dim_out:int):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention\n",
    "\n",
    "        :param dim_emb: 数据的嵌入维度\n",
    "        :param dim_out: 数据的输出维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        # 创建 Q、K、V 权重矩阵\n",
    "        self.Q_net = nn.Linear(dim_emb, dim_out, bias=False)\n",
    "        self.K_net = nn.Linear(dim_emb, dim_out, bias=False)\n",
    "        self.V_net = nn.Linear(dim_emb, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, mask_mat = None):\n",
    "\n",
    "        # 传入的 x_q、x_k、x_v 的维度为 (batch,seq_len,dim_emb)\n",
    "        # 先计算 q、k、v 三个值，它们的维度为 (batch,seq_len,dim_out)\n",
    "        q = self.Q_net(x_q)\n",
    "        k = self.K_net(x_k)\n",
    "        v = self.V_net(x_v)\n",
    "\n",
    "        # 值得注意的是 q 的维度为 (batch,seq_len,dim_out)，k 转置后的维度为 (batch,dim_out,seq_len)，\n",
    "        # 所以两者的矩阵乘法积 s 的维度为 (batch,seq_len,seq_len)，所以这里是一个方阵，就可以为接下来的矩阵\n",
    "        # 遮掩做准备了。\n",
    "        s = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.dim_out)\n",
    "\n",
    "        # 检查是否需要遮掩数据\n",
    "        if mask_mat is not None:\n",
    "            # 进行遮掩\n",
    "            s = torch.masked_fill(s, mask_mat, float('-inf'))\n",
    "\n",
    "        # 计算 softmax(s1)*v 并返回，返回的维度为 (batch,seq_len,dim_out)\n",
    "        return torch.matmul(F.softmax(s ,dim= -1), v).type_as(x_q)"
   ],
   "id": "cd54760300b6dac1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (2) 实现多头注意力（Multi-Head Attention）神经网络模块\n",
    "\n",
    "下面是多头注意力模块结构的示意图：\n",
    "\n",
    "![Multi-Head Attention](img/Multi-Head-Attention-Struct.svg)\n",
    "\n",
    "如上图所示，图中有 $N$ 个注意力模块，这里的 $N$ 就是 **多头注意力的头数** ，单个注意力的输出为 $out_{i}$ 。我们需要其直接拼接成一个大的矩阵：\n",
    "\n",
    "$$\n",
    "M = concat(out_{1},out_{2} ... ,out_{n})\n",
    "$$\n",
    "\n",
    "然后用拼接好的矩阵 $M$ 乘上一个权重矩阵 $W$ ，既为输出：\n",
    "\n",
    "$$\n",
    "Output = M \\times W\n",
    "$$\n",
    "\n",
    "其中单个注意力模块的 $W_{q}/W_{k}/W_{v}$ 维度为 $D_{emb} \\times D_{head}$，这里的 $D_{emb}$ 为输入 $X$ 的嵌入维度， $D_{head}$ 为 **头维度** ，这个值可以自行定义。"
   ],
   "id": "d3ffef713d2a42e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_emb:int, dim_head:int, dim_out:int, head_num:int):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "\n",
    "        :param dim_emb: 数据的嵌入维度\n",
    "        :param dim_head: 每个 Attention 块对应的头维度\n",
    "        :param dim_out: 数据的输出维度\n",
    "        :param head_num: 注意力头数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.dim_out = dim_out\n",
    "        self.head_num = head_num\n",
    "\n",
    "        # 构建 head_num 个注意力块\n",
    "        self.attention_blocks = nn.ModuleList()\n",
    "        for i in range(head_num):\n",
    "            self.attention_blocks.append(Attention(dim_emb, dim_head))\n",
    "\n",
    "        # 构建 W 权重矩阵\n",
    "        self.W_net = nn.Linear(dim_head * head_num, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, mask_mat = None):\n",
    "\n",
    "        # 合并所有注意力模块的输出，单个注意力块的输出为 (batch,seq_len,dim_head)，一共有 head_num 个\n",
    "        # 注意力块，所以所有注意力块合并在一起后，其维度为 (batch,seq_len,dim_head * head_num)\n",
    "        data = torch.concat(\n",
    "            [attention(x_q, x_k, x_v, mask_mat) for attention in self.attention_blocks],\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        # 将合并的输出乘上 W 矩阵，其维度为 (batch,seq_len,dim_out)\n",
    "        return self.W_net(data)"
   ],
   "id": "f3033ec9cea39cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 两个函数的实现\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Positional Encoding Function\n",
    "\n",
    "正弦-余弦位置编码函数，对于序列中的每个位置 $pos$、隐藏层维度 $d$ 、隐藏层中的每个维度索引 $i$ 、位置编码向量的第 $i$ 个元素\n",
    " $PE_{(pos,2i)}$ 和 $PE_{(pos,2i + 1)}$ 分别通过正弦和余弦函数计算：\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d}}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i + 1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d}}})\n",
    "$$\n",
    "\n",
    "实际上就是偶数采用 $sin$ 函数计算，奇数采用 $cos$ 函数计算。推荐阅读：[知乎-位置编码](https://zhuanlan.zhihu.com/p/580739030)"
   ],
   "id": "d98bbb4f8687e790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def positional_encoding(batch:int , seq_len:int, dim:int, dtype:torch.dtype = torch.float32) :\n",
    "    \"\"\"\n",
    "    位置编码\n",
    "\n",
    "    :param batch: 批次数量\n",
    "    :param seq_len: 序列长度\n",
    "    :param dim: 序列中单个数据的隐藏维度\n",
    "    :param dtype: 数据的类型\n",
    "    :return: 正弦-余弦位置编码\n",
    "    \"\"\"\n",
    "\n",
    "    batch_pe_mat = torch.zeros([batch, seq_len, dim], requires_grad=False, dtype=dtype)\n",
    "    for batch_idx in range(batch):\n",
    "        for pos in range(seq_len) :\n",
    "            for i in range(dim):\n",
    "                if i % 2 == 0:\n",
    "                    batch_pe_mat[batch_idx][pos][i] = torch.sin(torch.tensor(pos / (10000**(i/dim)), dtype= dtype, requires_grad=False))\n",
    "                else:\n",
    "                    batch_pe_mat[batch_idx][pos][i] = torch.cos(torch.tensor(pos / (10000**((i - 1)/dim)), dtype= dtype, requires_grad=False))\n",
    "\n",
    "    return batch_pe_mat"
   ],
   "id": "52903afd86b93367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Mask Matrix Generate Function\n",
    "\n",
    "掩码多头注意力中的掩码实现原理如下图所示：\n",
    "\n",
    "![Mask Mat Demo](img/Mask-Mat-Demo.svg)\n",
    "\n",
    "图中灰色的部分会被遮掩，在被遮掩后其值会被填充为 `-inf` （负无穷）。对于一个长度为 $n$ 需要遮掩的长度为 $n_{mask}$ 的序列，其对应掩码矩阵 $A_{n \\times n}$ 的任意元素 $a_{i,j}$ 有：\n",
    "\n",
    "$$\n",
    "a_{i,j} = \\begin{cases}\n",
    "1,& n - j + i \\leqslant n_{mask}\\\\\n",
    "0,& n - j + i > n_{mask}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "这里的 `1` 表达为 `true` 指需要被遮掩的元素，同理 `0` 表达为 `false` 指不需要被遮掩的元素 。"
   ],
   "id": "510615f063e9dd6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_mask_mat(seq_len, masked_len):\n",
    "    \"\"\"\n",
    "    生成掩码矩阵\n",
    "\n",
    "    :param seq_len: 序列长度\n",
    "    :param masked_len: 掩码长度\n",
    "    :return: 掩码矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    mask_mat = torch.zeros([seq_len, seq_len], dtype = torch.bool, requires_grad=False)\n",
    "\n",
    "    # 填充掩码矩阵\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            mask_mat[i][j] = torch.tensor(seq_len - j + i <= masked_len, dtype = torch.bool, requires_grad=False)\n",
    "\n",
    "    return mask_mat"
   ],
   "id": "c9a5ac527dfae991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer 最终实现\n",
    "\n",
    "---\n",
    "\n",
    "在 Transformer 结构中，可以将“详细原理图”中的左侧虚线框部分视作一个“编码器”（Encoder），将右侧虚线框内的部分视作一个“解码器”(Decoder)，其中 Encoder 数量与 Decoder 数量不必相等，那么整个 Transformer 的结构可以简化为下图：\n",
    "\n",
    "![Encoder Decoder Struct](img/Encoder-Decoder-Struce.svg)\n",
    "\n",
    "在 Transformer 中，输入张量通过 Encoder/Decoder 后要保持其维度不变，即输入维度为 `(batch,seq_len,dim_emb)` 从 Encoder/Decoder 输出后的维度也要为 `(batch,seq_len,dim_emb)` ，所以接下来实现的 Encoder 和 Decoder 都会令其中使用到的多头注意力机制的 `dim_out` 等于其 `dim_emb` 。"
   ],
   "id": "881f620eaf29e465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Encoder",
   "id": "9414f03347a7e3d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_emb:int, dim_head:int, head_num:int):\n",
    "        \"\"\"\n",
    "        Transformer Encoder\n",
    "\n",
    "        :param dim_emb: 数据的嵌入维度\n",
    "        :param dim_head: 单个注意力模块的头维度\n",
    "        :param head_num: 多头注意力的头数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.head_num = head_num\n",
    "\n",
    "        # 构建编码器需要的子模块\n",
    "        self.multi_head_attention = MultiHeadAttention(dim_emb, dim_head, dim_emb, head_num)\n",
    "        self.rms_norm1 = RMSNorm(dim_emb)\n",
    "        self.feed_forward = FeedForward(dim_emb, 4 * dim_emb)\n",
    "        self.rms_norm2 = RMSNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        d0 = self.multi_head_attention(x, x, x)\n",
    "        d1 = self.rms_norm1(x + d0)\n",
    "        d2 = self.feed_forward(d1)\n",
    "        d3 = self.rms_norm2(d1 + d2)\n",
    "\n",
    "        return d3"
   ],
   "id": "2f8f87c2c7fe5e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Decoder",
   "id": "135a08b9ab03619e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_emb:int, dim_head:int, head_num:int):\n",
    "        \"\"\"\n",
    "        Transformer Decoder\n",
    "\n",
    "        :param dim_emb: 数据的嵌入维度\n",
    "        :param dim_head: 单个注意力模块的头维度\n",
    "        :param head_num: 多头注意力的头数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.head_num = head_num\n",
    "\n",
    "        # 构建解码器需要的子模块\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(dim_emb, dim_head, dim_emb, head_num)\n",
    "        self.rms_norm1 = RMSNorm(dim_emb)\n",
    "        self.multi_head_attention = MultiHeadAttention(dim_emb, dim_head, dim_emb, head_num)\n",
    "        self.rms_norm2 = RMSNorm(dim_emb)\n",
    "        self.feed_forward = FeedForward(dim_emb, 4 * dim_emb)\n",
    "        self.rms_norm3 = RMSNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x, mask_mat = None, enc_k = None, enc_v = None):\n",
    "\n",
    "        d0 = self.masked_multi_head_attention(x, x, x, mask_mat)\n",
    "        d1 = self.rms_norm1(x + d0)\n",
    "\n",
    "        _enc_k = d1 if enc_k is None else enc_k\n",
    "        _enc_v = d1 if enc_v is None else enc_v\n",
    "\n",
    "        d2 = self.multi_head_attention(d1, _enc_k, _enc_v)\n",
    "        d3 = self.rms_norm2(d1 + d2)\n",
    "        d4 = self.feed_forward(d3)\n",
    "        d5 = self.rms_norm3(d3 + d4)\n",
    "\n",
    "        return d5"
   ],
   "id": "7b6d0e2cc1fc4451",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Transformer",
   "id": "c3426753db434558"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_num:int,\n",
    "            decoder_num:int,\n",
    "            vocab_size:int,\n",
    "            dim_emb:int,\n",
    "            dim_head:int,\n",
    "            head_num:int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer\n",
    "\n",
    "        :param encoder_num: 编码器数量\n",
    "        :param decoder_num: 解码器数量\n",
    "        :param vocab_size: 词表大小\n",
    "        :param dim_emb: 嵌入维度\n",
    "        :param dim_head: 单个注意力模块的头维度\n",
    "        :param head_num: 头数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_num = encoder_num\n",
    "        self.decoder_num = decoder_num\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_head = dim_head\n",
    "        self.head_num = head_num\n",
    "\n",
    "        # 词向量嵌入模块\n",
    "        self.input_embedding = nn.Embedding(vocab_size, dim_emb)\n",
    "        self.output_embedding = nn.Embedding(vocab_size, dim_emb)\n",
    "\n",
    "        # 创建解码器和编码器\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        for i in range(encoder_num):\n",
    "            self.encoder_blocks.append(Encoder(dim_emb, dim_head, head_num))\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        for i in range(decoder_num):\n",
    "            self.decoder_blocks.append(Decoder(dim_emb, dim_head, head_num))\n",
    "\n",
    "        # 创建线性层，线性层的作用是将嵌入维度转换为词表维度\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, output_seq, mask_mat = None):\n",
    "\n",
    "        # 输入词向量嵌入\n",
    "        embedded_input_seq = self.input_embedding(input_seq)\n",
    "\n",
    "        # 计算输入序列位置编码\n",
    "        batch,input_seq_len = input_seq.size()\n",
    "        input_positional_code = positional_encoding(batch, input_seq_len, self.dim_emb)\n",
    "\n",
    "        # 对 input_seq 进行位置标记\n",
    "        pe_input_seq = embedded_input_seq + input_positional_code\n",
    "\n",
    "        # 计算编码器的计算结果，其最终输出维度为 (batch, input_seq_len, dim_emb)\n",
    "        for encoder in self.encoder_blocks:\n",
    "            pe_input_seq = encoder(pe_input_seq)\n",
    "\n",
    "        # 输出词向量嵌入\n",
    "        embedded_output_seq = self.output_embedding(output_seq)\n",
    "\n",
    "        # 计算输出序列位置编码\n",
    "        _,output_seq_len = output_seq.size()\n",
    "        output_positional_code = positional_encoding(batch, output_seq_len, self.dim_emb)\n",
    "\n",
    "        # 对 output_seq 进行位置标记\n",
    "        pe_output_seq = embedded_output_seq + output_positional_code\n",
    "\n",
    "        # 计算解码器的计算结果，其最终输出维度为 (batch, output_seq_len, dim_emb)\n",
    "        for decoder in self.decoder_blocks:\n",
    "            pe_output_seq = decoder(pe_output_seq, mask_mat, pe_input_seq, pe_input_seq)\n",
    "\n",
    "        # 最终输出维度为 (batch, output_seq_len, vocab_size)\n",
    "        return F.softmax(self.linear(pe_output_seq), dim=-1)"
   ],
   "id": "bd33cf5015f871a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型生成分析\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 模型生成结果\n",
    "\n",
    "transformer 的生成结果是一个 `(batch, output_seq_len, vocab_size)` 大小的张量，我们抛去 `batch` 这一维度仅看 `(output_seq_len, vocab_size)` ，其中 `output_seq_len` 是**原始输入**的序列长度，`vocab_size` 是词库长度， 那么每一行对应的列就是改行对应的下一个单词的在词库中的分布概率。下面是生成过程示意图：\n",
    "\n",
    "![Transformer Prediction](img/Transformer-Prediction.svg)\n",
    "\n",
    "此图中，Transformer 的作用是要将一句英文内容翻译为中文内容，现在我们只需关注 Decoder 部分即可：原始输入 $X$ 在掩码矩阵`Mask Matrix` 的作用下将对应的元素进行遮蔽，比如图中 `Veiw Field` 的第一行遮蔽了“河之水天” 只留下了 “黄” 字的视野，而 transformer 最终生成的结果概率矩阵 `Probability Matrix` 对应的第一行就是预测 “黄” 字下一个 token 在词库中的概率。\n",
    "\n",
    "注意，上图只是一个简易的示意图，与其背后的真实原理有一定的差异，推荐参考：[BiliBili-利用上下文之三角掩码矩阵](https://www.bilibili.com/video/BV1cEPCesEKt)\n",
    "\n",
    "### 2. 模型预测\n",
    "\n",
    "在预测下一个 token 时，我们通常关注的是解码器生成的最后一个 token 的概率分布。因此，我们需要从 `(batch, output_seq_len, vocab_size)` 的概率分布中取出最后一个位置的概率分布，然后使用 `torch.argmax` 选择概率最高的 token（取 token 有很多采样策略，这里为了方便测试采用的是**贪婪策略**）。具体步骤如下：\n",
    "\n",
    "- 若 transformer 的最终输出为 $P(y_t)$ ，取出最后一个位置的概率分布：\n",
    "$$\n",
    "P(y_t)_{last} = P(y_t)[:,−1,:]\n",
    "$$\n",
    "\n",
    "  其中，$P(y_t)_{last}$ 的维度为 `(batch, vocab_size)`。\n",
    "\n",
    "- 使用 `torch.argmax` 选择概率最高的 token：\n",
    "<center>\n",
    "token_next = torch.argmax( $P(y_t)_{last}$ ,dim $=−1$ )\n",
    "</center><p/>\n",
    "\n",
    "  其中，`token_next` 的维度为 `(batch, 1)`。这样，我们就可以得到每个批次中下一个 token 的预测值。\n"
   ],
   "id": "e0d881181c818015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# 配置\n",
    "# =====================================\n",
    "encoder_num = 2  # 编码器数量\n",
    "decoder_num = 3  # 解码器数量\n",
    "vocab_size = 256 # 词表大小\n",
    "dim_emb = 128    # 词向量维度\n",
    "dim_head = 32    # 注意力头维度\n",
    "head_num = 4     # 注意力头数\n",
    "\n",
    "batch = 3             # 测试数据批数\n",
    "input_seq_len = 12    # 测试输入数据长度\n",
    "output_seq_len = 7    # 测试输出数据长度\n",
    "output_masked_len = output_seq_len - 1 # 测试输出掩码长度\n",
    "# =====================================\n",
    "\n",
    "# 创建 transformer 网络\n",
    "transformer = Transformer(encoder_num, decoder_num, vocab_size, dim_emb, dim_head, head_num)\n",
    "\n",
    "def rand_seq_code(batch, seq_len):\n",
    "    \"\"\"\n",
    "    随机序列编码\n",
    "\n",
    "    :param batch: 批数\n",
    "    :param seq_len: 序列长度\n",
    "    :return: 随机序列编码\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for i in range(batch):\n",
    "        bat_data = [random.randint(0, vocab_size) for _ in range(seq_len)]\n",
    "        data.append(bat_data)\n",
    "    return torch.tensor(data)\n",
    "\n",
    "# 构建随机输入输出序列\n",
    "input_seq = rand_seq_code(batch, input_seq_len)\n",
    "output_seq = rand_seq_code(batch, output_seq_len)\n",
    "\n",
    "# 构建掩码矩阵\n",
    "mask_mat = torch.stack([generate_mask_mat(output_seq_len, output_masked_len) for i in range(batch)])\n",
    "\n",
    "# transformer 输出\n",
    "res = transformer(input_seq, output_seq)\n",
    "\n",
    "# 打印输出结果\n",
    "print(res)\n",
    "print(res.size())\n",
    "\n",
    "# 预测下一个值\n",
    "next_token = torch.argmax(res[:,-1,:], dim=-1)\n",
    "print(next_token)"
   ],
   "id": "a35d5baff4c9362",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
